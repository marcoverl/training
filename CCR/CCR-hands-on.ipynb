{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark notebook examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the Spark Context\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code estimates π by \"throwing darts\" at a circle. We pick random points in the unit square ((0, 0) to (1,1)) and see how many fall in the unit circle. The fraction should be π / 4, so we use this to get our estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "NUM_SAMPLES = 100000000\n",
    "\n",
    "def inside(p):\n",
    "    x, y = random(), random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "             .filter(inside).count()\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD API example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize a data set converting from an Array to an RDD\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in the RDD\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some rows\n",
    "print(rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort descending\n",
    "descendingRdd = rdd.sortBy(lambda x: x, ascending = False)\n",
    "\n",
    "# View some rows\n",
    "print(descendingRdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the RDD\n",
    "filteredRdd = rdd.filter(lambda x: x < 5)\n",
    "\n",
    "# View some rows\n",
    "print(filteredRdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the RDD\n",
    "rdd2 = rdd.map(lambda x: (x, x * 2))\n",
    "\n",
    "# View some rows\n",
    "print(rdd2.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the RDD by adding up all of the numbers\n",
    "result = rdd.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Text file from HDFS / MINIO\n",
    "#textFile = sc.textFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an RDD to HDFS\n",
    "#textFile.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelize a data set converting from an Array to an RDD\n",
    "rdd = sc.parallelize([\"aaa bbb ccc\", \"aaa bbb\", \"bbb ccc\", \"abc\"])\n",
    "\n",
    "# WordCount\n",
    "results = rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get the Results\n",
    "results.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL adn DataFrame API examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the Spark Session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas\n",
    "data = [\n",
    "    (1, 'a'), \n",
    "    (2, 'b'), \n",
    "    (3, 'c'), \n",
    "    (4, 'd'), \n",
    "    (5, 'e'), \n",
    "    (6, 'a'), \n",
    "    (7, 'b'), \n",
    "    (8, 'c'), \n",
    "    (9, 'd'), \n",
    "    (10, 'e')\n",
    "]\n",
    "\n",
    "# Convert a local data set into a DataFrame\n",
    "df = spark.createDataFrame(data, ['numbers', 'letters'])\n",
    "\n",
    "# Convert to a Pandas DataFrame for easy display\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the DataFrame as a table\n",
    "df.registerTempTable(\"mytable\")\n",
    "\n",
    "# Perform a simple select from the table\n",
    "results = spark.sql(\"select * from mytable\")\n",
    "\n",
    "# Convert the results to a Pandas DataFrame for easy viewing\n",
    "results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a query with a where clause and order by\n",
    "results = spark.sql(\"select * from mytable where numbers < 8 order by numbers desc\")\n",
    "\n",
    "# Convert the results to a Pandas DataFrame for easy viewing\n",
    "results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a more complex query on the table\n",
    "results = spark.sql(\"select letters, count(*) as count, avg(numbers) as avg, sum(numbers) as sum from mytable group by letters\")\n",
    "\n",
    "# Convert the results to a Pandas DataFrame for easy viewing\n",
    "results.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "print(df.count())\n",
    "# View some rows\n",
    "print(df.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort descending\n",
    "descendingDf = df.orderBy(df.numbers.desc())\n",
    "\n",
    "# View some rows\n",
    "descendingDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "filtered = df.where(df.numbers < 5)\n",
    "\n",
    "# Convert to Pandas DataFrame for easy viewing\n",
    "filtered.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some more functions\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Perform aggregations on the DataFrame\n",
    "agg = df.agg(\n",
    "    avg(df.numbers).alias(\"avg_numbers\"), \n",
    "    sum(df.numbers).alias(\"sum_numbers\"),\n",
    "    countDistinct(df.numbers).alias(\"distinct_numbers\"), \n",
    "    countDistinct(df.letters).alias('distinct_letters')\n",
    ")\n",
    "\n",
    "# Convert the results to Pandas DataFrame\n",
    "agg.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some summary statistics\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Spark Context by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the current context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"k8s://https://kubernetes:443\")\n",
    "         .setAppName(\"MyApp\")\n",
    "         .set(\"spark.executor.memory\", \"1g\")\n",
    "         .set(\"spark.executor.instances\", \"2\")\n",
    "         .set(\"spark.kubernetes.container.image\", \"dodasts/spark:v3.0.0\")\n",
    "# configure S3 access         \n",
    "         .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://90.147.174.115:34900\")\n",
    "         .set(\"spark.hadoop.fs.s3a.path.style.access\",\"true\")\n",
    "         .set(\"spark.hadoop.fs.s3a.access.key\", \"CCR\")\n",
    "         .set(\"spark.hadoop.fs.s3a.secret.key\", \"tutorialCCR\")\n",
    "         .set(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "         .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "         .set(\"spark.hadoop.fs.s3a.committer.name\", \"directory\")\n",
    "       )\n",
    "\n",
    "sc = SparkContext(conf = conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a RDD from a text file         \n",
    "text_rdd = sc.textFile(\"s3a://democcr/CCR-Tutorial-Days.txt\")\n",
    "print(text_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count example         \n",
    "counts = text_rdd.flatMap(lambda line: line.split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b) \\\n",
    "             .map(lambda x: (x[1],x[0])) \\\n",
    "             .sortByKey(ascending = False)\n",
    "#counts.saveAsTextFile(\"s3a://democcr/out.txt\")\n",
    "counts.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from a CSV file         \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "csv_df = spark.read.csv(\"s3a://democcr/Arrivi-e-presenze-turistiche-serie-storica-2003-2016.csv\")\n",
    "csv_df.printSchema()\n",
    "csv_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV with some options         \n",
    "csv_df = spark.read.options(delimiter=';',header='True',inferSchema='True').csv(\"s3a://democcr/Arrivi-e-presenze-turistiche-serie-storica-2003-2016.csv\")\n",
    "csv_df.printSchema()\n",
    "csv_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the context (to free up resources)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l /usr/local/spark/bin/spark-submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /usr/local/spark/bin/spark-submit --master k8s://kubernetes:443 --deploy-mode cluster --name spark-pi \\\n",
    "        -c spark.driver.memory=2g -c spark.executor.instances=2 \\\n",
    "        -c spark.kubernetes.container.image=dodasts/spark:v3.0.0 \\\n",
    "        local:///usr/local/spark/examples/src/main/python/pi.py 5000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "sparkconnect": {
   "bundled_options": [],
   "list_of_options": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
